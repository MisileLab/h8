seed: 42
device: "auto"
train:
  steps: 5000
  batch_size: 65536  # Start large - will auto-reduce on OOM to find max usable
  lr: 1.0e-3
  weight_decay: 0.0
  lr_schedule: "constant"
  warmup_steps: 0
  amp: "bf16"  # Use mixed precision for better memory efficiency
  resume_path: null
  log_every: 100
  eval_every: 500
  save_every: 500
  save_every_steps: null
  eval_every_steps: null

  # Dynamic batch size - starts large and reduces on OOM to find optimal size
  dynamic_batch:
    enabled: true  # Auto-find max batch size via OOM retry
    min_batch_size: 64  # Won't go below this
    oom_reduction_factor: 0.7  # Reduce by 30% on each OOM
    max_oom_retries: 10  # Max retries before failing

  hard_negative:
    enabled: true
    mode: "teacher_tail"
    tail_from: 10
    tail_to: 100
    mix_random_ratio: 0.5
  rank:
    kind: "info_nce"
    multi_positive:
      enabled: true
      num_positives: 4
loss:
  distill: true
  distill_mode: "cosine"
  rank: true
  struct: true
  rank_temperature: 0.07
  alpha: 1.0
  beta: 0.5
  gamma: 0.5
  warmup_frac: 0.3
track_b:
  enable: false
  k_pos: 50
  m_neg: 1024
  tau: 0.07
  queue_size: 32000
  queue_cpu_fallback: true
  false_neg_filter:
    mode: "threshold"
    threshold: 0.8
    top_percent: 0.02
  mix:
    lambda: 1.0
eval:
  query_n: 2000
  k_values: [10, 100]
  backend: "hnsw"
  metric: "cosine"
  force_normalize: true
  use_two_stage: false
  candidate_n: 200
  m: 16
  ef_construction: 200
  ef_search: 64
