seed: 42
device: "auto"
train:
  steps: 5000
  batch_size: 256  # Initial batch size; use "auto" to auto-estimate from VRAM
  lr: 1.0e-3
  weight_decay: 0.0
  lr_schedule: "constant"
  warmup_steps: 0
  amp: "none"  # Use "fp16" or "bf16" to enable mixed precision (reduces memory)
  resume_path: null
  log_every: 100
  eval_every: 500
  save_every: 500
  save_every_steps: null
  eval_every_steps: null

  # Dynamic batch size configuration
  dynamic_batch:
    enabled: true  # Automatic batch size adjustment based on VRAM
    min_batch_size: 16  # Minimum batch size (won't go below this)
    max_batch_size: 4096  # Maximum batch size (won't exceed this)
    memory_fraction: 0.95  # Target fraction of VRAM to use (aggressive)
    oom_retry_enabled: true  # Retry on OOM with smaller batch
    oom_reduction_factor: 0.7  # Reduce batch by this factor on OOM
    max_oom_retries: 5  # Max number of OOM retries before failing
    warmup_steps: 10  # Steps before attempting batch size growth
    growth_factor: 1.1  # Factor to grow batch size if memory allows
    growth_check_interval: 50  # Check for growth every N steps

  hard_negative:
    enabled: true
    mode: "teacher_tail"
    tail_from: 10
    tail_to: 100
    mix_random_ratio: 0.5
  rank:
    kind: "info_nce"
    multi_positive:
      enabled: true
      num_positives: 4
loss:
  distill: true
  distill_mode: "cosine"
  rank: true
  struct: true
  rank_temperature: 0.07
  alpha: 1.0
  beta: 0.5
  gamma: 0.5
  warmup_frac: 0.3
track_b:
  enable: false
  k_pos: 50
  m_neg: 1024
  tau: 0.07
  queue_size: 32000
  queue_cpu_fallback: true
  false_neg_filter:
    mode: "threshold"
    threshold: 0.8
    top_percent: 0.02
  mix:
    lambda: 1.0
eval:
  query_n: 2000
  k_values: [10, 100]
  backend: "hnsw"
  metric: "cosine"
  force_normalize: true
  use_two_stage: false
  candidate_n: 200
  m: 16
  ef_construction: 200
  ef_search: 64
